import os
import onnxruntime as ort
from fastapi import FastAPI

app = FastAPI()

# Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n Ä‘áº¿n mÃ´ hÃ¬nh (file model.onnx náº±m á»Ÿ thÆ° má»¥c gá»‘c)
MODEL_PATH = os.path.join(os.getcwd(), "model.onnx")

# Debug: In ra Ä‘Æ°á»ng dáº«n file trÃªn Render
print(f"ğŸ” Äang tÃ¬m mÃ´ hÃ¬nh táº¡i: {MODEL_PATH}")

# Kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ tá»“n táº¡i khÃ´ng
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f" KhÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh táº¡i: {MODEL_PATH}")

# Load mÃ´ hÃ¬nh ONNX
try:
    ort_session = ort.InferenceSession(MODEL_PATH)
    print("âœ… MÃ´ hÃ¬nh ONNX Ä‘Ã£ Ä‘Æ°á»£c load thÃ nh cÃ´ng!")
except Exception as e:
    raise RuntimeError(f" Lá»—i khi load mÃ´ hÃ¬nh ONNX: {str(e)}")

@app.get("/")
def read_root():
    return {"message": " API cháº¡y thÃ nh cÃ´ng trÃªn Render!"}

@app.get("/predict")
def predict():
    return {"message": " Chá»©c nÄƒng dá»± Ä‘oÃ¡n chÆ°a Ä‘Æ°á»£c triá»ƒn khai"}

# Äoáº¡n nÃ y giÃºp Render cháº¡y uvicorn
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
