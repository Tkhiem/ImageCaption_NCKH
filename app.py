import os
import onnxruntime as ort
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from PIL import Image
import numpy as np
from io import BytesIO

app = FastAPI()

# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n ƒë·∫øn m√¥ h√¨nh ONNX (model.onnx n·∫±m trong th∆∞ m·ª•c g·ªëc c·ªßa project)
MODEL_PATH = os.getenv("MODEL_PATH", "./model.onnx")

# Ki·ªÉm tra xem m√¥ h√¨nh c√≥ t·ªìn t·∫°i kh√¥ng
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh t·∫°i: {MODEL_PATH}")

# Load m√¥ h√¨nh ONNX
try:
    ort_session = ort.InferenceSession(MODEL_PATH)
except Exception as e:
    raise RuntimeError(f"L·ªói khi load m√¥ h√¨nh ONNX: {str(e)}")

@app.get("/")
def read_root():
    return {"message": "API ch·∫°y th√†nh c√¥ng tr√™n Render!"}

@app.get("/predict")
def get_predict():
    return {"message": "Ch·ª©c nƒÉng d·ª± ƒëo√°n ch∆∞a ƒë∆∞·ª£c tri·ªÉn khai (GET)"}

# H√†m x·ª≠ l√Ω ƒë·∫ßu ra c·ªßa m√¥ h√¨nh
def process_output(ort_outs):
    # Chuy·ªÉn ƒë·∫ßu ra th√†nh NumPy array n·∫øu ch∆∞a ph·∫£i
    if isinstance(ort_outs, list):
        ort_outs = np.array(ort_outs)
    
    print("üìå Shape c·ªßa output:", ort_outs.shape)
    print("üìå Gi√° tr·ªã ƒë·∫ßu ra:", ort_outs)
    print("üìå Ki·ªÉu d·ªØ li·ªáu:", ort_outs.dtype)
    
    return ort_outs

# Endpoint POST /predict ƒë·ªÉ nh·∫≠n file ·∫£nh v√† tr·∫£ v·ªÅ ch√∫ th√≠ch
@app.post("/predict/")
async def predict(image: UploadFile = File(...)):
    try:
        # ƒê·ªçc n·ªôi dung file ·∫£nh t·ª´ client
        contents = await image.read()
        img = Image.open(BytesIO(contents)).convert("RGB")
        img = img.resize((224, 224))  # Resize ·∫£nh theo y√™u c·∫ßu m√¥ h√¨nh
        
        # Chuy·ªÉn ƒë·ªïi ·∫£nh sang m·∫£ng NumPy
        input_array = np.array(img).astype(np.float32) / 255.0
        input_array = np.transpose(input_array, (2, 0, 1))  # Chuy·ªÉn t·ª´ (H, W, C) -> (C, H, W)
        input_array = np.expand_dims(input_array, axis=0)  # Th√™m batch dimension
        
        # Chu·∫©n b·ªã input cho m√¥ h√¨nh ONNX
        input_name = ort_session.get_inputs()[0].name
        ort_inputs = {input_name: input_array}

        # G·ªçi suy lu·∫≠n (inference) c·ªßa m√¥ h√¨nh
        ort_outs = ort_session.run(None, ort_inputs)
        processed_output = process_output(ort_outs[0])
        
        # TODO: X·ª≠ l√Ω ƒë·∫ßu ra c·ªßa m√¥ h√¨nh ƒë·ªÉ chuy·ªÉn th√†nh chu·ªói ch√∫ th√≠ch
        caption = "Dummy caption - implement decoding logic here"

        return JSONResponse(content={"caption": caption})
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# Ch·∫°y ·ª©ng d·ª•ng n·∫øu t·ªáp ƒë∆∞·ª£c ch·∫°y tr·ª±c ti·∫øp
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
